# 决策树

## 1.1 决策树基本模型

&emsp;决策树(decision tree) 是一种基本的**分类**与**回归**方法。

&emsp;在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是 if-then 规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其**优点**是模型具有可读性，分类速度快。

&emsp;决策树的学习通常包括 3 个步骤：特征选择、决策树的生成、决策树的剪枝。

**分类决策树基本模型**

&emsp;分类决策树模型是一个描述对实例进行分类的树形结构，由结点(node) 和有向边(directed edge) 组成。结点有两种类型：内部结点（表示一个特征或属性），叶结点（表示一个类别）

**决策树与 if-then 规则**

&emsp;路径上内部结点和特征对应着规则的条件，而叶结点的类别对应着规则的结论。

&emsp;决策树的路径或其对应的 if-then 规则集合具有一个重要的性质：**互斥并且完备**，即每一个实例都被一条路径所覆盖，而且只被一条路径所覆盖。

**决策树与条件概率分布**

&emsp;决策树树还表示给定特征条件下类的条件概率分布P(Y|X)，这一条件概率分布定义在特征空间的一个划分(partition) 上，将特征空间划分为互不相交的单元(cell) 或区域(region)， 并在每个单元定义一个类的概率分布就构成了一个条件概率分布。

&emsp;决策树的一条路径对应于划分中的一个单元。各叶结点（单元）上的条件概率往往偏向于某一个类，即属于某一类的概率较大，决策树分类时将该结点的实例强行分到条件概率大的那一类。

**决策树的学习**

&emsp;决策树学习用损失函数表示这一目标。当损失函数确定后，学习问题就变为在损失函数意义下选择最优决策树的问题。决策树学习算法通常采用启发式方法，近似求解这一最优化问题，这样得到的决策树是次优的(sub-optimal)

&emsp;决策树学习的算法通常是一个递归地选择最优特征，这一过程对应着特征空间的划分，也对应着决策树的构建。

&emsp;决策树的学习需要解决两个关键问题：

1. 何时划分？该问题决定了决策树构建的目标函数。

2. 何时停止划分？

## 1.2 ID3 算法

&emsp;ID3 决策树使用信息增益作为划分的标准，每次选择信息增益最大的特征（使条件熵变小）。是一种自顶向下的贪心算法。

&emsp;优点：方法简单，理论清晰

&emsp;缺点：

1. 只能处理离散数据，不能处理连续数据

2. 信息增益的缺点是倾向于选择取值较多的属性，在有些情况下这类属性可能不会提供太多有价值的信息。

3. ID3 算法只有树的生成，容易过拟合

&emsp;ID3 算法使用信息增益(information gain) 来决定何时划分，其表示得知特征X 的信息而使得类Y 的信息的不确定性减少的程度。

&emsp;设特征A 对训练数据集D 的信息增益 infoGain(D,A) 定义如下
```python
infoGain(D, A) = H(D) - H(D|A)
```
H()表示熵，信息增益定义为集合D 的经验熵H(D) 与在特征A 条件下D 的经验条件熵H(D|A) 之差。一般地，熵H(Y) 与条件熵H(Y|X) 之差成为互信息(mutual information)。信息增益等价于训练数据集中类与特征的互信息。

&emsp;ID3 算法的采用两个自然的终止条件：

1. 当划分只剩下一类时（即无需划分），终止划分，剩下的类别为该划分的类别。

2. 当划分的特征只有一种值（即无法划分时），终止划分，使用多数投票法选择该划分中数量最多的类别作为该划分的类别。