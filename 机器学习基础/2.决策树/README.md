# 决策树

## 1.1 决策树基本模型

&emsp;决策树(decision tree) 是一种基本的**分类**与**回归**方法。

&emsp;在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是 if-then 规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其**优点**是模型具有可读性，分类速度快。

&emsp;决策树的学习通常包括 3 个步骤：特征选择、决策树的生成、决策树的剪枝。

**分类决策树基本模型**

&emsp;分类决策树模型是一个描述对实例进行分类的树形结构，由结点(node) 和有向边(directed edge) 组成。结点有两种类型：内部结点（表示一个特征或属性），叶结点（表示一个类别）

**决策树与 if-then 规则**

&emsp;路径上内部结点和特征对应着规则的条件，而叶结点的类别对应着规则的结论。

&emsp;决策树的路径或其对应的 if-then 规则集合具有一个重要的性质：**互斥并且完备**，即每一个实例都被一条路径所覆盖，而且只被一条路径所覆盖。

**决策树与条件概率分布**

&emsp;决策树树还表示给定特征条件下类的条件概率分布P(Y|X)，这一条件概率分布定义在特征空间的一个划分(partition) 上，将特征空间划分为互不相交的单元(cell) 或区域(region)， 并在每个单元定义一个类的概率分布就构成了一个条件概率分布。

&emsp;决策树的一条路径对应于划分中的一个单元。各叶结点（单元）上的条件概率往往偏向于某一个类，即属于某一类的概率较大，决策树分类时将该结点的实例强行分到条件概率大的那一类。

**决策树的学习**

&emsp;决策树学习用损失函数表示这一目标。当损失函数确定后，学习问题就变为在损失函数意义下选择最优决策树的问题。决策树学习算法通常采用启发式方法，近似求解这一最优化问题，这样得到的决策树是次优的(sub-optimal)

&emsp;决策树学习的算法通常是一个递归地选择最优特征，这一过程对应着特征空间的划分，也对应着决策树的构建。

## 1.2 ID3 算法
